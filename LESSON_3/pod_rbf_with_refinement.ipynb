{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/mathLab/EZyRB\n",
    "!mkdir data\n",
    "!wget -P data https://github.com/giovastabile/roma_phd_course_exercises/raw/master/LESSON_1/data/tut1_coord.npy\n",
    "!wget -P data https://github.com/giovastabile/roma_phd_course_exercises/raw/master/LESSON_1/data/tut1_mu.npy\n",
    "!wget -P data https://github.com/giovastabile/roma_phd_course_exercises/raw/master/LESSON_1/data/tut1_snapshots.npy\n",
    "!wget -P data https://github.com/giovastabile/roma_phd_course_exercises/raw/master/LESSON_1/data/tut1_triangles.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.tri as mtri\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ezyrb import POD, RBF, Database\n",
    "from ezyrb import ReducedOrderModel as ROM\n",
    "import numpy.linalg as LA\n",
    "\n",
    "from scipy.spatial import Delaunay\n",
    "\n",
    "# ## Offline phase\n",
    "# \n",
    "# In the *offline* phase, we need some samples of the parametric high-fidelity model. In this case, we extract 8 snapshots from the numerical model implemented in **FEniCS**, and we import them and the related parameters.\n",
    "\n",
    "\n",
    "snapshots = np.load('data/tut1_snapshots.npy')\n",
    "param = np.load('data/tut1_mu.npy')\n",
    "\n",
    "print(snapshots.shape, param.shape)\n",
    "\n",
    "train_snapshots = snapshots[0:8,:]\n",
    "train_param = param[0:8,:]\n",
    "print(train_snapshots.shape, train_param.shape)\n",
    "\n",
    "# Moreover, to visualize the solution (both the higher-order one and the reduced one), we import also the mesh information to be able to create the triangulation. We underline this additional step is related only to plotting purpose, and not mandatory for the reduced space generation.\n",
    "\n",
    "\n",
    "tri = np.load('data/tut1_triangles.npy')\n",
    "coord = np.load('data/tut1_coord.npy')\n",
    "triang = mtri.Triangulation(coord[0],coord[1],tri)\n",
    "\n",
    "\n",
    "# For the sake of clarity the snapshots are plotted.\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(16, 6), sharey=True, sharex=True)\n",
    "ax = ax.flatten()\n",
    "for i in range(8):\n",
    "    ax[i].triplot(triang, 'b-', lw=0.1)\n",
    "    cm = ax[i].tripcolor(triang, snapshots[i])\n",
    "    fig.colorbar(cm, ax=ax[i])\n",
    "    ax[i].set_title('($\\mu_0={:5.2f}, \\mu_1={:5.2f})$'.format(*param[i]))\n",
    "\n",
    "plt.show()\n",
    "# First of all, we create a `Database` object from the parameters and the snapshots.\n",
    "\n",
    "db = Database(train_param, train_snapshots)\n",
    "\n",
    "\n",
    "\n",
    "# Then we need a reduction object. In this case we use the proper orthogonal decomposition so we create a `POD` object. We use here all the default parameters, but for the complete list of available arguments we refer to original documentation of [POD](https://mathlab.github.io/EZyRB/pod.html) class.\n",
    "\n",
    "pod = POD('svd')\n",
    "\n",
    "\n",
    "# Then we instantiate the `RBF` class for interpolating the solution manifold. Also in this case, [RBF](https://mathlab.github.io/EZyRB/rbf.html) documentation is the perfect starting point to explore such class.\n",
    "\n",
    "\n",
    "rbf = RBF()\n",
    "\n",
    "\n",
    "# Few lines of code and our reduced model is created!\n",
    "# To complete everything, we create the `ReducedOrderModel` (aliased to `ROM` in this tutorial) object by passing the already created objects. For clarity, we puntualize that we need to pass the **instances** and not the classes. Simply changing such line (with different objects) allows to test different frameworks in a very modular way.\n",
    "# The `fit()` function computes the reduced model, meaning that the original snapshots in the database are projected onto the POD space and the RBF interpolator is created.\n",
    "\n",
    "rom = ROM(db, pod, rbf)\n",
    "rom.fit();\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(train_param[:,0],train_param[:,1],'*')\n",
    "\n",
    "plt.title('Train vs test parameter location')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "# ## Error Approximation & Improvement\n",
    "# \n",
    "# At the moment, we used a database which is composed by 8 files. we would have an idea of the approximation accuracy we are able to reach with these high-fidelity solutions. Using the *leave-one-out* strategy, an error is computed for each parametric point in our database and these values are returned as array. \n",
    "\n",
    "\n",
    "sample_space_tria = Delaunay(train_param)\n",
    "loo_error = []\n",
    "print('Leave one out error')\n",
    "for pt, error in zip(rom.database.parameters, rom.loo_error()):\n",
    "    print(pt, error)\n",
    "    loo_error.append(error)\n",
    "    \n",
    "error = np.array(loo_error)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Moreover, we can use the information about the errors to locate the parametric points where we have to compute the new high-fidelity solutions and add these to the database in order to optimally improve the accuracy.\n",
    "\n",
    "\n",
    "mu_opt = rom.optimal_mu()\n",
    "print('Optimal parameter for enrichment')\n",
    "print(mu_opt)\n",
    "\n",
    "ps_triang = mtri.Triangulation(train_param[:,0],train_param[:,1],sample_space_tria.simplices)\n",
    "plt.triplot(ps_triang, 'b-', lw=0.1)\n",
    "plt.tripcolor(ps_triang, error)\n",
    "plt.plot(train_param[:,0],train_param[:,1],'*')\n",
    "plt.plot(mu_opt[:,0],mu_opt[:,1],'*')\n",
    "plt.colorbar()\n",
    "plt.title('Leave one out error')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# These function can be used to achieve the wanted (estimated) accuracy.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
